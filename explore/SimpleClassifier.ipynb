{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We create tabular data based on some feature extracted from the observations\n",
    "and fit a simple classifier to it (Random Forest). Using 1000 paths it seems do distinguish the following different\n",
    "pairs of volatility coeff:\n",
    "    sigma1 = 0.1, sigma2 = 0.8 with accuracy 0.96\n",
    "    sigma1 = 0.2, sigma2 = 0.7 with accuray 0.89\n",
    "    sigma1 = 0.3, sigma2 = 0.6 with accuracy 0.78\n",
    "    sigma1 = 0.4, sigma2 = 0.5 with accuracy 0.45\n",
    "    sigma1 = 0.3, sigma2 = 0.5 with accuracy 0.68\n",
    "Has to be runned in the NJODE foder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import matplotlib\n",
    "import matplotlib.colors\n",
    "from torch.backends import cudnn\n",
    "import gc\n",
    "sys.path.append(\"../\")\n",
    "try:\n",
    "    from . import models as models\n",
    "    from . import data_utils as data_utils\n",
    "    from ..GRU_ODE_Bayes import models_gru_ode_bayes as models_gru_ode_bayes\n",
    "except Exception:\n",
    "    import NJODE.models as models\n",
    "    import NJODE.data_utils as data_utils\n",
    "    import GRU_ODE_Bayes.models_gru_ode_bayes as models_gru_ode_bayes\n",
    "import matplotlib.pyplot as plt\n",
    "import stock_model as stock_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create mixed Dataset\n",
    "#change the values of volatility for tests\n",
    "stock_model_names = ('BlackScholes', 'BlackScholes')\n",
    "hyperparam_vol_1 = {\n",
    "    'drift': 2., 'volatility': 0.3, 'mean': 4,\n",
    "    'speed': 2., 'correlation': 0.5, 'nb_paths': 1000, 'nb_steps': 100,\n",
    "    'S0': 1, 'maturity': 1., 'dimension': 1, \n",
    "    'obs_perc': 0.1,\n",
    "    'scheme': 'euler', 'return_vol': False, 'v0': 1,\n",
    "}\n",
    "hyperparam_vol_2 = {\n",
    "    'drift': 2., 'volatility': 0.5, 'mean': 4,\n",
    "    'speed': 2., 'correlation': 0.5, 'nb_paths': 1000, 'nb_steps': 100,\n",
    "    'S0': 1, 'maturity': 1., 'dimension': 1, \n",
    "    'obs_perc': 0.1,\n",
    "    'scheme': 'euler', 'return_vol': False, 'v0': 1,\n",
    "}\n",
    "path1, time_id1 = data_utils.create_dataset('BlackScholes',hyperparam_vol_1)\n",
    "path2, time_id2 = data_utils.create_dataset('BlackScholes',hyperparam_vol_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = 'BlackScholes'\n",
    "dataset2 = 'BlackScholes'\n",
    "\n",
    "data1 = data_utils.IrregularDataset(model_name=dataset1, time_id=time_id1)\n",
    "data2 = data_utils.IrregularDataset(model_name=dataset2, time_id=time_id2)\n",
    "\n",
    "dl1 = DataLoader(dataset = data1, collate_fn = data_utils.custom_collate_fn,shuffle  = False, batch_size = 1, num_workers = 1)\n",
    "dl2 = DataLoader(dataset = data2, collate_fn = data_utils.custom_collate_fn,shuffle  = False, batch_size = 1, num_workers = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration with dataloader is not needed, it would probably be faster to work with the whole dataset\n",
    "\n",
    "data_tab = np.zeros((2*hyperparam_vol_1['nb_paths'],8))\n",
    "for j,batch in enumerate(dl1):\n",
    "        \n",
    "        #plot one trajectory\n",
    "        #plt.figure(0)\n",
    "        #plt.plot(np.linspace(0,1,101),batch['true_paths'][0,0,:])\n",
    "        \n",
    "        \n",
    "        #values of observed data\n",
    "        X = batch['X']\n",
    "        \n",
    "        #1 if we observed the data at this point in time 0 o.w.\n",
    "        observed_dates = batch['observed_dates'][0]\n",
    "        \n",
    "        #grid\n",
    "        path_t_true_X = np.linspace(0,1,101)\n",
    "\n",
    "        ## Add tabular features\n",
    "        #index of the time point were we observe data\n",
    "        ind = np.nonzero(observed_dates)\n",
    "        \n",
    "        #avoid cases with only 1 or 0 observation\n",
    "        if not(X.size()[0]<=1):\n",
    "            #jumps between the observation\n",
    "            jumps = np.diff(np.squeeze(X))\n",
    "            \n",
    "            #index for the largest jump\n",
    "            max_ind = np.where(jumps==max(jumps))[0]\n",
    "            \n",
    "            #difference in time for the largest jump\n",
    "            max_time_diff  = path_t_true_X[ind][max_ind]-path_t_true_X[ind][max_ind-1]\n",
    "\n",
    "            #same as above but for the smallest jump\n",
    "            min_ind = np.where(jumps == min(jumps))[0]\n",
    "            min_time_diff = path_t_true_X[ind][min_ind]-path_t_true_X[ind][min_ind-1]\n",
    "            \n",
    "            \n",
    "            mean_jump = np.mean(jumps)\n",
    "            \n",
    "            mean_time_diff = np.mean(path_t_true_X[ind])\n",
    "            \n",
    "            approx_qv = np.sum(np.square(jumps))\n",
    "            \n",
    "            #add the feature generated above + rescale max and min with time diff (Large jump in short time may be more important than large jump in large time)\n",
    "            data_tab[j,:] = [max(jumps),max(jumps)/max_time_diff,min(jumps),min(jumps)/min_time_diff,mean_jump,mean_time_diff,approx_qv,0]\n",
    "        else:\n",
    "            data_tab[j,:] = [0,0,0,0,0,0,0,0]\n",
    "        \n",
    "\n",
    "#as above but for dataset with volatility 2\n",
    "for j,batch in enumerate(dl2):\n",
    "        #plt.figure(1)\n",
    "        #plt.plot(np.linspace(0,1,101),batch['true_paths'][0,0,:])\n",
    "        X = batch['X']\n",
    "        observed_dates = batch['observed_dates'][0]\n",
    "        path_t_true_X = np.linspace(0,1,101)\n",
    "\n",
    "        ## Add tabular features\n",
    "        ind = np.nonzero(observed_dates)\n",
    "        if not(X.size()[0]<=1):\n",
    "\n",
    "            jumps = np.diff(np.squeeze(X))\n",
    "\n",
    "            max_ind = np.where(jumps==max(jumps))[0]\n",
    "            max_time_diff  = path_t_true_X[ind][max_ind]-path_t_true_X[ind][max_ind-1]\n",
    "\n",
    "\n",
    "            min_ind = np.where(jumps == min(jumps))[0]\n",
    "            min_time_diff = path_t_true_X[ind][min_ind]-path_t_true_X[ind][min_ind-1]\n",
    "\n",
    "            mean_jump = np.mean(jumps)\n",
    "            mean_time_diff = np.mean(path_t_true_X[ind])\n",
    "            approx_qv = np.sum(np.square(jumps))\n",
    "\n",
    "            data_tab[1000+j,:] = [max(jumps),max(jumps)/max_time_diff,min(jumps),min(jumps)/min_time_diff,mean_jump,mean_time_diff,approx_qv,1]\n",
    "        else:\n",
    "            data_tab[1000+j,:] = [0,0,0,0,0,0,0,0]\n",
    "            \n",
    "data_tab_df =  pd.DataFrame(data_tab)\n",
    "data_tab_df.to_csv('./data_tab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## Fit a simple classifier to the above data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_tab[:,0:7],data_tab[:,7],test_size = 0.3,shuffle = True)\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "accuracy = np.mean(clf.predict(X_test)==y_test)\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
